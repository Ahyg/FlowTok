experiment:
  project: radar_flowtitok_ae_bl77_vae
  name: radar_flowtitok_ae_bl77_vae_run1
  output_dir: /mnt/ssd_1/yghu/Experiments/radar_flowtitok_ae_bl77_vae_run1
  max_train_examples: 10464
  save_every: 50000
  eval_every: 50000
  generate_every: 5000
  log_every: 50
  log_grad_norm_every: 1000
  resume: true
  logging_dir: /mnt/ssd_1/yghu/Experiments/radar_flowtitok_ae_bl77_vae_run1/logs

model:
  vq_model:
    quantize_mode: vae
    token_size: 16
    vit_enc_model_size: base
    vit_dec_model_size: large
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 77
    in_channels: 1
    out_channels: 1
    finetune_decoder: false
    is_legacy: false

losses:
  discriminator_start: 400000
  quantizer_weight: 1.0
  discriminator_factor: 1.0
  discriminator_weight: 0.05
  perceptual_loss: lpips-convnext_s-1.0-0.1
  perceptual_weight: 0.2
  reconstruction_loss: l2
  reconstruction_weight: 1.0
  lecam_regularization_weight: 0.001
  kl_weight: 1.0e-06
  logvar_init: 0.0

dataset:
  params:
    filelist_path: /mnt/ssd_1/yghu/Data/71_3m/dataset_filelist.pkl
    filelist_split: train
    mode: radar
    use_lightning: false
    num_workers: 4
  preprocessing:
    resize_shorter_edge: 128
    crop_size: 128
    random_crop: true
    random_flip: true
    res_ratio_filtering: true

optimizer:
  name: adamw
  params:
    learning_rate: 0.0001
    discriminator_learning_rate: 0.0001
    beta1: 0.9
    beta2: 0.999
    weight_decay: 0.0001

lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05

training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 8
  mixed_precision: bf16
  enable_tf32: true
  enable_wandb: false
  use_ema: true
  seed: 42
  max_train_steps: 650000
  num_generated_images: 2
  max_grad_norm: 1.0
  nan_check: true
  nan_skip: true

